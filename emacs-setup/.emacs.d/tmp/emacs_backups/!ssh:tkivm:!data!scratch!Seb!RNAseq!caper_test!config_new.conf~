include required(classpath("application"))

backend {
  default = "Local"
  providers {

    slurm_singularity {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        script-epilogue = "sleep 30"
        concurrent-job-limit = 50
        runtime-attributes = """
          Int cpu = 1
          Int? gpu
          Int? time
          Int? memory_mb
          String? slurm_partition
          String? slurm_account
          String? slurm_extra_param
          String singularity_container
          String? singularity_options
          String singularity_command = "exec"
          String? singularity_command_options
        """
        submit = """
        # Ensure singularity is loaded if it's installed as a module
        # module load Singularity/3.0.1

# Build the Docker image into a singularity image
# NAME=echo ${docker}.sif | sed 's/\//_/g'
# NAME=$(echo ${docker}.sif    | sed 's/\//_/g')
# IMAGE=${cwd}/$NAME
# singularity build $IMAGE docker://${docker}

if [ -z $SINGULARITY_CACHEDIR ];
then CACHE_DIR=$HOME/.singularity/cache
else CACHE_DIR=$SINGULARITY_CACHEDIR
fi
# Make sure cache dir exists so lock file can be created by flock
mkdir -p $CACHE_DIR
LOCK_FILE=$CACHE_DIR/singularity_pull_flock
# Create an exclusive filelock with flock. --verbose is useful for
# for debugging, as is the echo command. These show up in `stdout.submit`.
flock --verbose --exclusive --timeout 900 $LOCK_FILE \
singularity exec --containall docker://${docker} \
echo "successfully pulled ${docker}!"

echo "$IMAGE"
# Submit the script to SLURM
sbatch \
--wait \
-J ${job_name} \
-D ${cwd} \
-o ${cwd}/execution/stdout \
-e ${cwd}/execution/stderr \
-t ${runtime_minutes} \
${true="--cpus-per-task=" false="" defined(cpu)}${cpu}  \
${true="--mem=" false="" defined(memory_mb)}${memory_mb}  \
${true="--mem-per-cpu=" false="" defined(requested_memory_mb_per_core)}${requested_memory_mb_per_core}  \
--wrap "singularity exec --containall --bind ${cwd}:${docker_cwd} --pwd ${docker_cwd} docker://${docker}  ${job_shell} execution/script"
                             """

                             kill = "scancel ${job_id}"
                             check-alive = "squeue -j ${job_id}"
                             job-id-regex = "Submitted batch job (\\d+).*"
      }
    }

    sge_singularity {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        script-epilogue = "sleep 30 && sync"
        concurrent-job-limit = 50
        runtime-attributes = """
          String sge_pe = "shm"
          Int cpu = 1
          Int? gpu
          Int? time
          Int? memory_mb
          String? sge_queue
          String? sge_extra_param
          String singularity_container
          String? singularity_options
          String singularity_command = "exec"
          String? singularity_command_options
        """
        submit = """
          echo "chmod u+x ${script} && SINGULARITY_BINDPATH=$(echo ${cwd} | sed 's/cromwell-executions/\n/g' | head -n1) singularity ${singularity_options} ${singularity_command} ${singularity_command_options} ${singularity_container} ${script}" | qsub \
          -terse \
          -b n \
          -N ${job_name} \
          -wd ${cwd} \
          -o ${out} \
          -e ${err} \
          ${if cpu>1 then "-pe " + sge_pe + " " + cpu else " "} \
          ${"-l h_vmem=" + memory_mb/cpu + "m"} \
          ${"-l s_vmem=" + memory_mb/cpu + "m"} \
          ${"-l h_rt=" + time*3600} \
          ${"-l s_rt=" + time*3600} \
          ${"-q " + sge_queue} \
          ${"-l gpu=" + gpu} \
          ${sge_extra_param} \
          -V
        """
        kill = "qdel ${job_id}"
        check-alive = "qstat -j ${job_id}"
        job-id-regex = "(\\d+)"
      }
    }

    singularity {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        script-epilogue = "sleep 5 && sync"
        concurrent-job-limit = 10
        run-in-background = true
        runtime-attributes = """
          Int? gpu
          String singularity_container
          String? singularity_bindpath
        """
        submit = """
          ${if defined(singularity_container) then "" else "/bin/bash ${script} #"} \
            if [ -z $SINGULARITY_BINDPATH ]; then SINGULARITY_BINDPATH=/; fi; \
            singularity exec --cleanenv --home ${cwd} \
            ${if defined(gpu) then '--nv' else ''} \
            ${singularity_container} /bin/bash ${script}
        """
      }
    }

    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 10
      }
    }

    sge {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        script-epilogue = "sleep 30 && sync"
        concurrent-job-limit = 50
        runtime-attributes = """
        String sge_pe = "shm"
        Int cpu = 1
        Int? gpu
        Int? time
        Int? memory_mb
        String? sge_queue
        String? sge_extra_param
        """
        submit = """
        qsub \
        -terse \
        -b n \
        -N ${job_name} \
        -wd ${cwd} \
        -o ${out} \
        -e ${err} \
        ${if cpu>1 then "-pe " + sge_pe + " " + cpu else " "} \
        ${"-l h_vmem=" + memory_mb/cpu + "m"} \
        ${"-l s_vmem=" + memory_mb/cpu + "m"} \
        ${"-l h_rt=" + time*3600} \
        ${"-l s_rt=" + time*3600} \
        ${"-q " + sge_queue} \
        ${"-l gpu=" + gpu} \
        ${sge_extra_param} \
        -V \
        ${script}
        """
        kill = "qdel ${job_id}"
        check-alive = "qstat -j ${job_id}"
        job-id-regex = "(\\d+)"
      }
    }

    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        script-epilogue = "sleep 30"
        concurrent-job-limit = 50
        runtime-attributes = """
        Int cpu = 1
        Int? gpu
        Int? time
        Int? memory_mb
        String? slurm_partition
        String? slurm_account
        String? slurm_extra_param
        """
        submit = """
        sbatch \
        --export=ALL \
        -J ${job_name} \
        -D ${cwd} \
        -o ${out} \
        -e ${err} \
        ${"-t " + time*60} \
        -n 1 \
        --ntasks-per-node=1 \
        ${"--cpus-per-task=" + cpu} \
        ${"--mem=" + memory_mb} \
        ${"-p " + slurm_partition} \
        ${"--account " + slurm_account} \
        ${"--gres gpu:" + gpu} \
        ${slurm_extra_param} \
        --wrap "/bin/bash ${script}"
        """
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }

    google {
      actor-factory = "cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory"
      config {
        # Google project
        project = "your-project-name"
        # Base bucket for workflow executions
        root = "gs://your-bucket-name"

        concurrent-job-limit = 1000
        genomics-api-queries-per-100-seconds = 1000
        maximum-polling-interval = 600

        genomics {
          auth = "application-default"
          compute-service-account = "default"
          endpoint-url = "https://genomics.googleapis.com/"
          restrict-metadata-access = false
        }

        filesystems {
          gcs {
            auth = "application-default"
          }
        }
      }
    }
  }
}

services {
  LoadController {
    class = "cromwell.services.loadcontroller.impl.LoadControllerServiceActor"
    config {
      # disable it (for login nodes on Stanford SCG, Sherlock)
      control-frequency = 21474834 seconds
    }
  }
}

system {
  abort-jobs-on-terminate = true
  graceful-server-shutdown = true
}

call-caching {
  enabled = false
  invalidate-bad-cache-results = true
}

database {
  db {
    connectionTimeout = 6000
  }
}

google {
  application-name = "cromwell"
  auths = [
    {
      name = "application-default"
      scheme = "application_default"
    }
  ]
}
