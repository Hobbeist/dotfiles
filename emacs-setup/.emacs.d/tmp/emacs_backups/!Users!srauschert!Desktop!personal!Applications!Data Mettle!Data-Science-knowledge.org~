#+TITLE:Some Data Science Knowledge 
#+AUTHOR: Sebastian Rauschert, PhD
#+email: Sebastian.Rauschert@telethonkids.org.au
#+INFOJS_OPT: 
#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
-----
* Machine Learning
** Model Metrics
*** Recall and Recall
_Recall_ attempts to answer the question: "What proportion of actual positives was identified correctly?"  

\[Precision = \frac{TP}{TP + FN}\]

_Precision_ attempts to answer the question: "What proportion of positive identifications was actually correct ?"

\[Recall = \frac{TP}{TP + FP}\]
*** False Positives and False Negatives
- A *false positive* is an incorrect identification of the presence of a condition, when it is absent.

- A *false negative* is an incorrect identification of the absence of a condition, when it is actually present .

  - A *false negative* can be more important than a *false positive*, when screening for cancer. It is much worse to say that someone does
    not have cancer, when they do, instead of saying that someone does and later realizing they do not.
*** Adjusted R^2 in multiple linear regression
R^2 is a measurement that tells you to what extent the proportion of variance in the dependent variable is explained by the variance in the independent variables. In simpler terms, while the coefficients estimate trends, R^2 represents the scatter around the line of best fit.

However, every additional independent variable added to a model always increases the R^2 value — therefore, a model with several independent variables may seem to be a better fit even if it isn’t. This is where adjusted R^2 comes in. The adjusted R^2 compensates for each additional independent variable and only increases if each given variable improves the model above what is possible by probability. This is important since we are creating a multiple regression model.

